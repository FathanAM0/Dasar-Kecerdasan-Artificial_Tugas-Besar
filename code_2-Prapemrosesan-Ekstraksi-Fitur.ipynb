{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a96dfd1",
   "metadata": {},
   "source": [
    "Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d57a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import joblib\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import scipy.fftpack\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import resample_poly\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187d8f4",
   "metadata": {},
   "source": [
    "Konstanta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcdd51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 48000\n",
    "SEGMENT_DURATION = int(1 * SAMPLE_RATE)\n",
    "OVERLAP_DURATION = int(0.5 * SAMPLE_RATE)\n",
    "\n",
    "MAIN_PATH = r\"C:\\Users\\Lulay\\Documents\\GitHub\\Dasar-Kecerdasan-Artificial_Tugas-Besar\"\n",
    "# MAIN_PATH = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f8148",
   "metadata": {},
   "source": [
    "Dir Folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74ad776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_xc = MAIN_PATH + r\"\\Dataset\\xeno-canto\"\n",
    "dir_data_nad = MAIN_PATH + r\"\\Dataset\\noise-audio-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c237f",
   "metadata": {},
   "source": [
    "Dir Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca398046",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_dataset = MAIN_PATH + r\"\\Dataset\\dataset_labelled.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0794a03a",
   "metadata": {},
   "source": [
    "# Pra-Pemrosesan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35c09a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_audio(path):\n",
    "    audio, sr = sf.read(path)\n",
    "    return np.array(audio), sr\n",
    "\n",
    "def prapemrosesan_downmixing(audio):\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "    return audio.astype(np.float32)\n",
    "\n",
    "def prapemrosesan_resampling(audio, sr):\n",
    "    if sr == SAMPLE_RATE:\n",
    "        return audio.copy(), SAMPLE_RATE\n",
    "    \n",
    "    ratio = SAMPLE_RATE / sr\n",
    "    n_samples = int(np.round(len(audio) * ratio))\n",
    "    \n",
    "    x_old = np.linspace(0, 1, len(audio))\n",
    "    x_new = np.linspace(0, 1, n_samples)\n",
    "    return np.interp(x_new, x_old, audio), SAMPLE_RATE\n",
    "\n",
    "def prapemrosesan_padding(audio):\n",
    "    if np.mod(audio.shape[0], SEGMENT_DURATION) != 0:\n",
    "        padding = SEGMENT_DURATION - (audio.shape[0] % SEGMENT_DURATION)\n",
    "        audio = np.pad(audio, (0, padding))\n",
    "    return audio\n",
    "\n",
    "def prapemrosesan_splitting(audio):\n",
    "    num_segments = int(np.floor((len(audio) - SEGMENT_DURATION) / OVERLAP_DURATION)) + 1\n",
    "    segments = []\n",
    "\n",
    "    for i in range(num_segments):\n",
    "        start = int(i * OVERLAP_DURATION)\n",
    "        end = int(start + SEGMENT_DURATION)\n",
    "        segment = audio[start:end]\n",
    "        if len(segment) < SEGMENT_DURATION:\n",
    "            segment = np.pad(segment, (0, SEGMENT_DURATION - len(segment)), mode='constant')\n",
    "        segments.append(segment)\n",
    "\n",
    "    return np.array(segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64547cbc",
   "metadata": {},
   "source": [
    "# Ekstraksi Fitur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a8759ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rms(segment):\n",
    "    return np.sqrt(np.mean(segment ** 2))\n",
    "\n",
    "def get_zcr(segment):\n",
    "    return np.sum(np.abs(np.diff(np.signbit(segment)))) / (len(segment) / SAMPLE_RATE)\n",
    "\n",
    "def get_lms(segment):\n",
    "    return librosa.feature.melspectrogram(y=segment, sr=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184bf31f",
   "metadata": {},
   "source": [
    "# Visualize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21519cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_plot(path, list_result, final_time):\n",
    "    file_name = os.path.basename(path)\n",
    "    audio, sr = librosa.load(path, sr=SAMPLE_RATE)\n",
    "    time_axis = np.linspace(0, len(audio) / sr, len(audio))\n",
    "\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.plot(time_axis, audio, label=\"Amplitude\")\n",
    "    plt.title(f\"Audio waveform with loud segments marked - {file_name}\") \n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "\n",
    "    for i in enumerate(list_result):\n",
    "        start_time = i[1][0]\n",
    "        end_time = i[1][1]\n",
    "        plt.axvspan(start_time, end_time, color='red', alpha=0.3)\n",
    "\n",
    "    plt.xticks(np.arange(0, final_time, 0.5))\n",
    "    plt.grid(which='both', alpha=0.5)\n",
    "    plt.xlim(0, final_time)\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b9211",
   "metadata": {},
   "source": [
    "# Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd379893",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{MAIN_PATH}/Dataset/dataset_labelled.pkl\", \"rb\") as f:\n",
    "    dataset_labelled = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505b090",
   "metadata": {},
   "source": [
    "# Ekstraksi Fitur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfb96bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C:\\\\Users\\\\Lulay\\\\Documents\\\\GitHub\\\\Dasar-Kecerdasan-Artificial_Tugas-Besar\\\\Dataset\\\\xeno-canto\\\\19655.mp3',\n",
       "  [0.0, 1.0],\n",
       "  2),\n",
       " ('C:\\\\Users\\\\Lulay\\\\Documents\\\\GitHub\\\\Dasar-Kecerdasan-Artificial_Tugas-Besar\\\\Dataset\\\\xeno-canto\\\\19655.mp3',\n",
       "  [0.5, 1.5],\n",
       "  2),\n",
       " ('C:\\\\Users\\\\Lulay\\\\Documents\\\\GitHub\\\\Dasar-Kecerdasan-Artificial_Tugas-Besar\\\\Dataset\\\\xeno-canto\\\\19655.mp3',\n",
       "  [1.0, 2.0],\n",
       "  2)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_labelled[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c33c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ef = []\n",
    "audio_cache = \"\"\n",
    "\n",
    "for audio_file, timestamp, label in dataset_labelled:\n",
    "    if audio_file != audio_cache:\n",
    "        audio_cache = audio_file\n",
    "        audio, sr = load_file_audio(audio_file)\n",
    "        audio = prapemrosesan_downmixing(audio)\n",
    "        audio, sr = prapemrosesan_resampling(audio, sr)\n",
    "        audio = prapemrosesan_padding(audio)\n",
    "        segments = prapemrosesan_splitting(audio)\n",
    "\n",
    "    index = timestamp[0] / 0.5\n",
    "    segment = segments[int(index)]\n",
    "\n",
    "    rms = get_rms(segment)\n",
    "    zcr = get_zcr(segment)\n",
    "    lms = get_lms(segment)\n",
    "    \n",
    "    dataset_ef.append([audio_file, timestamp, label, rms, zcr, lms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a87a6f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ef = []\n",
    "audio_cache = \"\"\n",
    "\n",
    "for audio_file, timestamp, label in dataset_labelled:\n",
    "    if audio_file != audio_cache:\n",
    "        audio_cache = audio_file\n",
    "        audio, sr = load_file_audio(audio_file)\n",
    "        audio = prapemrosesan_downmixing(audio)\n",
    "        audio, sr = prapemrosesan_resampling(audio, sr)\n",
    "        audio = prapemrosesan_padding(audio)\n",
    "        segments = prapemrosesan_splitting(audio)\n",
    "    \n",
    "    dataset_ef.append([audio_file, timestamp, label, segments])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173d050c",
   "metadata": {},
   "source": [
    "# Save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "932ac157",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{MAIN_PATH}/Dataset/dataset_preprocessed.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset_ef, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
